[eval]
prompt = """
Given an instruction and two responses—one generated by a human and the other by a language model—I'm seeking to evaluate how closely the language model's response mirrors the human-generated one. Additionally, I want to assess the accuracy and relevance of the language model's response to the original instruction.

Instruction:
```
$instruction
``` 
Human Response:
```
$human_response
```

Language Model Response:
```
$lm_response
```

You are quality assessor who analyzes the similarity between the Human Response and the Language Model Response on a scale of 1 to 100, where 1 indicates no similarity and 100 indicates identical responses.
Also you analyze the Language Model Response how it accurately answers the given Instruction on a scale of 1 to 100. Analysis MUST be rigorous and thorough.
Provide the assessment in the following JSON format:

{
  "similarity_assessment": {"score": [Insert similarity score here],"reason": [Insert how the similarity score is determined]},
  "precision_assessment": {"score": [Insert precision score here],"reason": [Insert how the precision score is determined]}
}
"""

[Report_eval]
prompt = """
The task involves evaluating a section of a report corrected by an open source language model based on the provided instruction. The evaluation should focus on relevance, completeness, and grammatical correctness, syntax correctness, and orthographic correctness, and another score to see if the corrected response doesn't change the technical domain-specific terms and proper names as presented in the instruction and also an OPENXML score to measure how effectively it maintains the same Open XML structure as the instruction.

Instruction:
```
$instruction
```
Generated Report Section:
```
$generated_section
```

You are a quality assessor tasked with analyzing the generated section based on the following criteria:

1. **Relevance**: Determine how relevant the section is to the instruction provided. Score between 0 (not relevant) and 20 (highly relevant).
2. **Completeness**: Consider whether the section comprehensively covers the required aspects of the instruction. Score between 0 (not complete) and 20 (fully comprehensive).
3. **Grammar and Style**: Note any grammatical or stylistic errors. Score between 0 (numerous errors) and 20 (flawless).
4. **Technical Accuracy**: Evaluate if the corrected response retains the technical domain-specific terms and proper names as presented in the instruction. Score between 0 (significant changes) and 20 (no changes).
5. **Open XML Structure**: Measure how effectively the corrected response maintains the same Open XML structure as the instruction. Score between 0 (significant changes) and 20 (no changes).

Provide the assessment in the following JSON format:

{
  "relevance": {"score": [Insert relevance score here], "reason": [Insert reason for relevance score]},
  "completeness": {"score": [Insert completeness score here], "reason": [Insert reason for completeness score]},
  "grammar_and_style": {"score": [Insert grammar and style score here], "reason": [Insert reason for grammar and style score]},
  "technical_accuracy": {"score": [Insert technical accuracy score here], "reason": [Insert reason for technical accuracy score]},
  "openxml_structure": {"score": [Insert openxml structure score here], "reason": [Insert reason for HTML structure score]}
}
"""


[synth_data_gen]
prompt = """
Generate a series of (instruction, response) pairs that are similar in context and structure to the example provided below. Each pair should consist of a concise instruction followed by an appropriate, detailed response. The instruction should pose a clear task or question, while the response should provide a comprehensive answer or solution that could be understood by someone with a basic understanding of the subject.

Example pair:

Instruction: $instruction
Response: $response

Your task is to generate more pairs that maintain this level of clarity and detail. Ensure that the responses are informative and accurate, suitable for an educational context.

Store the generated pairs in JSON format, with each pair as an object within an array. Each object should have two key-value pairs: "instruction" and "response". For instance:

{"contents": [{"instruction": "text", "response": "text"}, {"instruction": "text", "response": "text"}, ...]}

Remember to maintain consistency in the format and ensure the generated pairs are diverse and cover a broad range of subjects. You must return the response in the asked format and you must not add any additional text in your response.
"""